{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqR5txF-Me1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANuGS0oBBeUC",
        "colab_type": "code",
        "outputId": "b5acad97-cab8-433b-ed41-aa0199a7e6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd gdrive/My Drive/m3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "/content/gdrive/My Drive/m3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3K3lMuPKNNp",
        "colab_type": "code",
        "outputId": "7a221fc6-777b-4e3a-9c1b-c71bb7771418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import literate\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Lambda\n",
        "from keras.models import load_model\n",
        "import keras\n",
        "\n",
        "syldict = literate.syldict(\"Syllable_dictionary.txt\")\n",
        "data, words = literate.read(\"shakespeare.txt\", syldict)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTXqn2GcKNNv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def listwords_to_line(listwords):\n",
        "    return \" \".join(listwords)\n",
        "\n",
        "def lines_to_sonnet(lines):\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def listlist_to_sonnet(listlist):\n",
        "    return lines_to_sonnet(list(map(listwords_to_line, listlist)))\n",
        "\n",
        "def subthings(sonnet, n):\n",
        "    a = []\n",
        "    for i in np.arange(0,len(sonnet)-n):\n",
        "        a.append(sonnet[i:i+n])\n",
        "    return a\n",
        "\n",
        "def sublabels(sonnet, n):\n",
        "    a = []\n",
        "    for i in np.arange(0,len(sonnet)-n):\n",
        "        a.append(sonnet[i+n])\n",
        "    return a\n",
        "\n",
        "def flatten(a):\n",
        "    return [item for sublist in a for item in sublist]\n",
        "\n",
        "def split(word): \n",
        "    return [char for char in word]\n",
        "\n",
        "def string_to_ints(s):\n",
        "    return list(map(lambda x : char_to_int[x],split(s)))\n",
        "\n",
        "def get_model(temp,n_lstm,X,y):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(n_lstm, input_shape=(X.shape[1], X.shape[2])))\n",
        "    model.add(Lambda(lambda x: x / temp))\n",
        "    model.add(Dense(vocab_size, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "def train(model,n,use_adam,lr, X,y):\n",
        "    #lr = 0.001\n",
        "    adam = keras.optimizers.Adam(lr=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
        "\n",
        "    #lr =0.01\n",
        "    sgd = keras.optimizers.SGD(lr=lr, momentum=0.0, nesterov=False)\n",
        "\n",
        "    if(use_adam):\n",
        "        opt = adam\n",
        "    else:\n",
        "        opt = sgd\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    model.fit(X, y, epochs=n, verbose=2)\n",
        "    return model\n",
        "\n",
        "def gen(model, seed, n):\n",
        "    k = len(seed)\n",
        "    s = seed\n",
        "    for i in range(n):\n",
        "        s += predict_next(model,s[-k:])\n",
        "    return s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvlKmXGiKNNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getXy(sonnets):\n",
        "    listofsubsonnetlists = list(map(lambda x : subthings(x, s), sonnets))\n",
        "    listoflabellists = list(map(lambda x : sublabels(x, s), sonnets))\n",
        "    listofsubsonnets = flatten(listofsubsonnetlists)\n",
        "    listoflabels = flatten(listoflabellists)\n",
        "    intcharsubsonnets = list(map(string_to_ints, listofsubsonnets))\n",
        "    intcharlabels = list(map(lambda x : char_to_int[x],listoflabels))\n",
        "    X = intcharsubsonnets\n",
        "    y = intcharlabels\n",
        "    sequences = list(map(intstring_to_encoding,X))\n",
        "    X = np.array(sequences)\n",
        "    y = to_categorical(y, num_classes=vocab_size)\n",
        "    return X,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B26iRxXDfm-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def intstring_to_encoding(su):\n",
        "    return to_categorical(su, num_classes=vocab_size)\n",
        "\n",
        "def string_to_encoding(s):\n",
        "    return intstring_to_encoding(string_to_ints(s))\n",
        "\n",
        "def predict_next(m,s):\n",
        "    encoded = np.array([string_to_encoding(s)])\n",
        "    yhats = m.predict(encoded, verbose=0)[0]\n",
        "    return np.random.choice(chars,p=yhats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNItXhmPVZqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do(lr,n,use_adam,n_lstm,temp,name,X,y):\n",
        "    model = get_model(temp,n_lstm, X,y)\n",
        "    model = train(model,n,use_adam,lr,X,y)\n",
        "    model.save(name)\n",
        "    return model\n",
        "\n",
        "def execp(p,X,y):\n",
        "    return do(p[0],p[1],p[2],p[3],p[4],p[5],X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gg9SyQIzwmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read(fn):\n",
        "    f = open(fn,\"r\")\n",
        "    contents = f.read()\n",
        "    return contents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxX1ziHj-C55",
        "colab_type": "code",
        "outputId": "53585489-6bb2-4dc7-c577-ec636bd6d344",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "def gen(pf, seed, n):\n",
        "    k = len(seed)\n",
        "    s = seed\n",
        "    for i in range(n):\n",
        "        s += pf(s[-k:])\n",
        "    return s\n",
        "\n",
        "def is_prefix(p,s):\n",
        "    return s.startswith(p)\n",
        "\n",
        "def is_valid_so_far(word,words):\n",
        "    for w in words:\n",
        "        if is_prefix(word, w):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "#seed ends in a space\n",
        "def add_next_word(model, start, words, k, max_i):\n",
        "    i=0\n",
        "    while True:\n",
        "        s=start\n",
        "        w=\"\"\n",
        "        while True:\n",
        "            i+=1\n",
        "            if i > max_i:\n",
        "                return \"blah\"\n",
        "            p = predict_next(model,s[-k:])\n",
        "            s += p\n",
        "            if not is_valid_so_far(w,words):\n",
        "                print(w)\n",
        "                break\n",
        "            if (p == \" \" or p == \"\\n\") and w in words:\n",
        "               return s\n",
        "            w += p\n",
        "\n",
        "def cleanup(s):\n",
        "    s2 = \"\"\n",
        "    for c in s:\n",
        "        lc = c.lower()\n",
        "        if lc in chars:\n",
        "            s2 += lc   \n",
        "    return s2\n",
        "\n",
        "!pip install pronouncing\n",
        "import pronouncing\n",
        "def get_rhymes(word):\n",
        "    return list(map(cleanup,pronouncing.rhymes(word)))\n",
        "\n",
        "def syllables(word):\n",
        "    count = 0\n",
        "    vowels = 'aeiouy'\n",
        "    word = word.lower()\n",
        "    if word[0] in vowels:\n",
        "        count +=1\n",
        "    for index in range(1,len(word)):\n",
        "        if word[index] in vowels and word[index-1] not in vowels:\n",
        "            count +=1\n",
        "    if word.endswith('e'):\n",
        "        count -= 1\n",
        "    if word.endswith('le'):\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "import nltk\n",
        "nltk.download('cmudict')\n",
        "from nltk.corpus import cmudict\n",
        "d = cmudict.dict()\n",
        "\n",
        "def num_syllables(word):\n",
        "    try:\n",
        "        return ([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])[0]\n",
        "    except KeyError:\n",
        "        #if word not found in cmudict\n",
        "        return syllables(word)\n",
        "\n",
        "import random\n",
        "def random_word_from(w,words):\n",
        "    halfvalid = get_rhymes(w)\n",
        "    valid = list(np.intersect1d(np.array(halfvalid,dtype=\"object\"), np.array(list(words),dtype=\"object\")))\n",
        "    if len(valid)==0:\n",
        "        if len(halfvalid)==0:\n",
        "            return random.sample(words,1)[0]\n",
        "        else:\n",
        "            return random.sample(halfvalid,1)[0]\n",
        "    return random.sample(valid,1)[0]\n",
        "\n",
        "def add_next_rhyming_word(pf, start, k, words, max_i, rhyme, must_rhyme):\n",
        "    i=0\n",
        "    while True:\n",
        "        s=start\n",
        "        w=\"\"\n",
        "        while True:\n",
        "            i+=1\n",
        "            if i > max_i:\n",
        "                w = random_word_from(rhyme, words)\n",
        "                print(w)\n",
        "                return \"{}{}\\n\".format(start,w), w\n",
        "            p = pf(s[-k:])#predict_next(model,s[-k:])\n",
        "            s += p\n",
        "\n",
        "            if p == \" \":\n",
        "                if not w in words:\n",
        "                    break\n",
        "                if must_rhyme:\n",
        "                    break\n",
        "                return s, w\n",
        "            if p == \"\\n\" and rhyme == \"\":\n",
        "                if not w in words:\n",
        "                    break\n",
        "                return s, w\n",
        "            if p == \"\\n\":\n",
        "                if not w in words:\n",
        "                    break\n",
        "                if w in get_rhymes(rhyme):\n",
        "                    return s, w\n",
        "                else:\n",
        "                    break\n",
        "            w += p\n",
        "            if not is_valid_so_far(w,words):\n",
        "                break\n",
        "\n",
        "def sensegen(model, seed, nw, words, max_i):\n",
        "    k=len(seed)\n",
        "    s = seed\n",
        "    for i in range(nw):\n",
        "        s = add_next_word(model, s, words, k, max_i)\n",
        "    return s\n",
        "\n",
        "def rhymegen(pf, seed, lines, words, max_i, firstrhyme,first_line, numsylreq):\n",
        "    none = \"\"\n",
        "    endword1=firstrhyme\n",
        "    endword2=\"\"\n",
        "    k=len(seed)\n",
        "    s = seed\n",
        "    line = first_line\n",
        "    numsyl=0\n",
        "    while True:\n",
        "        if line >= lines:\n",
        "            break\n",
        "        rhyme = [none, none, endword2, endword2, none, none, endword2, endword2, none,none,endword2, endword2, none, endword1][line]\n",
        "        if numsyl >= numsylreq-1:\n",
        "            s,w = add_next_rhyming_word(pf, s, k, words, max_i, rhyme, True)\n",
        "        else:\n",
        "            s,w = add_next_rhyming_word(pf, s, k, words, max_i, rhyme, False)\n",
        "        numsyl+=num_syllables(w)\n",
        "        \n",
        "        if s[-1] == \"\\n\":\n",
        "            print(line)\n",
        "            line += 1\n",
        "            endword2 = endword1\n",
        "            endword1 = w\n",
        "            numsyl=0\n",
        "        \n",
        "    return s\n",
        "\n",
        "basicseed = \"e\\nshall i compare thee to a summers day\\n\"\n",
        "def test_model(pf, words):\n",
        "    normal = gen(pf, basicseed,1000)\n",
        "    rhyme = rhymegen(pf, basicseed, 14, words, 1000,\"day\",1,10)\n",
        "    print(normal)\n",
        "    print(rhyme)\n",
        "\n",
        "def pf(model):\n",
        "    def pff(x):\n",
        "        return predict_next(model,x)\n",
        "    return pff\n",
        "\n",
        "def prob(p):\n",
        "    return random.uniform(0, 1) < p\n",
        "\n",
        "def pf2(m1,m2,p):\n",
        "    def pf2f(x): \n",
        "        if prob(p): \n",
        "            return predict_next(m1,x) \n",
        "        else:\n",
        "            return predict_next(m2,x) \n",
        "    return pf2f"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pronouncing\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/c6/9dc74a3ddca71c492e224116b6654592bfe5717b4a78582e4d9c3345d153/pronouncing-0.2.0.tar.gz\n",
            "Collecting cmudict>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/cf/4d24ac4f3ea5a57406a690ad7c07023c310185eac99adae7473c9ebdf550/cmudict-0.4.4-py2.py3-none-any.whl (938kB)\n",
            "\u001b[K     |████████████████████████████████| 942kB 4.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pronouncing\n",
            "  Building wheel for pronouncing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pronouncing: filename=pronouncing-0.2.0-py2.py3-none-any.whl size=6223 sha256=14a78de0d87f8da58bdf0554bbdb382a9367f0d7baf3935bef22053250be58c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/fd/e8/fb1a226f707c7e20dbed4c43f81b819d279ffd3b0e2f06ee13\n",
            "Successfully built pronouncing\n",
            "Installing collected packages: cmudict, pronouncing\n",
            "Successfully installed cmudict-0.4.4 pronouncing-0.2.0\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iR7EAv51-o6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sonnets = list(map(listlist_to_sonnet, data))\n",
        "onestring=\"\".join(sonnets)\n",
        "chars=list(np.sort(np.unique(split(onestring))))\n",
        "vocab_size = len(chars)\n",
        "ints = range(len(chars))\n",
        "char_to_int = dict(zip(chars, ints))\n",
        "int_to_char = dict(zip(ints, chars))\n",
        "\n",
        "s=40\n",
        "\n",
        "coro = read(\"coro.txt\")\n",
        "coro2 = cleanup(coro)\n",
        "\n",
        "XS,yS = getXy(sonnets)\n",
        "XC,yC = getXy([coro2])\n",
        "\n",
        "import re\n",
        "\n",
        "wordsS = list(words)\n",
        "wordsC = list(set(re.split(\" |\\n\",coro2)))\n",
        "wordsC.remove(\"\")\n",
        "wordsC.remove(\"e\")\n",
        "wordsC.remove(\"m\")\n",
        "wordsC.remove(\"n\")\n",
        "wordsC.remove(\"s\")\n",
        "wordsC.remove(\"b\")\n",
        "wordsSC = list(np.union1d(np.array(wordsS,dtype=\"object\"),np.array(wordsC,dtype=\"object\")))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdEyhMxICNxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "dbdc7773-e030-4a89-b2f7-daac1bb15ad1"
      },
      "source": [
        "norm = [0.001,50,True,100,1,\"norm.h5\"]\n",
        "cor1 = [0.001,50,True,100,1,\"cor1.h5\"]\n",
        "tune = [0.05,45,False,150,1,\"tune.h5\"]\n",
        "tp25 = [0.001,50,True,100,0.25,\"tp25.h5\"]\n",
        "tp75 = [0.001,50,True,100,0.75,\"tp75.h5\"]\n",
        "t1p5 = [0.001,50,True,100,1.5,\"t1p5.h5\"]\n",
        "\n",
        "norm_m = load_model(\"norm.h5\")\n",
        "coro_m = load_model(\"cor1.h5\")\n",
        "tp25_m = load_model(\"tp25.h5\")\n",
        "tp75_m = load_model(\"tp75.h5\")\n",
        "t1p5_m = load_model(\"t1p5.h5\")\n",
        "tune_m = load_model(\"tune.h5\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsaICBpu8KzE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perp(gensonnet,trainsonnet):\n",
        "    Xt,yt = getXy([trainsonnet])\n",
        "    Xr,yr = getXy([gensonnet])\n",
        "    print(np.shape(yt))\n",
        "    print(np.shape(yr))\n",
        "    return perplexity(K.constant(yt),K.constant(yr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_XvPhaVpZLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "import math\n",
        "def perplexity(y_true, y_pred):\n",
        "    cross_entropy = K.categorical_crossentropy(K.constant(y_true), K.constant(y_pred))\n",
        "    perplexity = K.exp(cross_entropy)\n",
        "    return K.eval(perplexity)\n",
        "def randomsubstring(s,n):\n",
        "  x = random.randint(0,len(s)-n)\n",
        "  return s[x:x+n]\n",
        "def num_sub(s,sub):\n",
        "  return s.count(sub)\n",
        "def nums_subs(s,subs):\n",
        "  return list(map(lambda x : num_sub(s,x), subs))\n",
        "def todist(a):\n",
        "  return np.divide(a,np.sum(a))\n",
        "def dist_subs(s,subs):\n",
        "  return todist(nums_subs(s,subs))\n",
        "def nperp(s_true,s_pred,n,gn):\n",
        "  grams = list(map(lambda x: randomsubstring(s_true,n),range(gn)))\n",
        "  d_true = dist_subs(s_true,grams)\n",
        "  d_pred = dist_subs(s_pred,grams)\n",
        "  perp = perplexity(d_true,d_pred)\n",
        "  return perp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nI-_m_qAkqp",
        "colab_type": "code",
        "outputId": "ef1d2a3e-9df3-43f8-e670-e37dd4c92720",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Xt,yt = getXy(sonnets)\n",
        "norm_m.test_on_batch(Xt,yt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.99728376, 0.68284065]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HaAz5Rk-2KI",
        "colab_type": "code",
        "outputId": "1d78fac1-90ab-4e3b-edb6-91aa15392d8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#norm_m = execp(norm,XS,yS)\n",
        "#coro_m = execp(cor1,XC,yC)\n",
        "#tp25_m = execp(tp25,XS,yS)\n",
        "#tp75_m = execp(tp75,XS,yS)\n",
        "#t1p5_m = execp(t1p5,XS,yS)\n",
        "#tune_m = execp(tune,XS,yS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            " - 22s - loss: 2.7778 - acc: 0.1892\n",
            "Epoch 2/50\n",
            " - 22s - loss: 2.3383 - acc: 0.3283\n",
            "Epoch 3/50\n",
            " - 22s - loss: 2.1719 - acc: 0.3562\n",
            "Epoch 4/50\n",
            " - 22s - loss: 2.0681 - acc: 0.3873\n",
            "Epoch 5/50\n",
            " - 22s - loss: 1.9811 - acc: 0.4140\n",
            "Epoch 6/50\n",
            " - 23s - loss: 1.9047 - acc: 0.4428\n",
            "Epoch 7/50\n",
            " - 22s - loss: 1.8349 - acc: 0.4673\n",
            "Epoch 8/50\n",
            " - 22s - loss: 1.7663 - acc: 0.4884\n",
            "Epoch 9/50\n",
            " - 22s - loss: 1.7037 - acc: 0.5104\n",
            "Epoch 10/50\n",
            " - 22s - loss: 1.6456 - acc: 0.5272\n",
            "Epoch 11/50\n",
            " - 22s - loss: 1.5895 - acc: 0.5415\n",
            "Epoch 12/50\n",
            " - 22s - loss: 1.5407 - acc: 0.5575\n",
            "Epoch 13/50\n",
            " - 22s - loss: 1.4904 - acc: 0.5704\n",
            "Epoch 14/50\n",
            " - 22s - loss: 1.4422 - acc: 0.5868\n",
            "Epoch 15/50\n",
            " - 22s - loss: 1.3973 - acc: 0.5967\n",
            "Epoch 16/50\n",
            " - 22s - loss: 1.3520 - acc: 0.6133\n",
            "Epoch 17/50\n",
            " - 22s - loss: 1.3078 - acc: 0.6239\n",
            "Epoch 18/50\n",
            " - 22s - loss: 1.2697 - acc: 0.6338\n",
            "Epoch 19/50\n",
            " - 22s - loss: 1.2271 - acc: 0.6489\n",
            "Epoch 20/50\n",
            " - 22s - loss: 1.1903 - acc: 0.6557\n",
            "Epoch 21/50\n",
            " - 22s - loss: 1.1514 - acc: 0.6659\n",
            "Epoch 22/50\n",
            " - 22s - loss: 1.1175 - acc: 0.6744\n",
            "Epoch 23/50\n",
            " - 22s - loss: 1.0832 - acc: 0.6844\n",
            "Epoch 24/50\n",
            " - 22s - loss: 1.0490 - acc: 0.6958\n",
            "Epoch 25/50\n",
            " - 22s - loss: 1.0165 - acc: 0.7058\n",
            "Epoch 26/50\n",
            " - 22s - loss: 0.9876 - acc: 0.7146\n",
            "Epoch 27/50\n",
            " - 22s - loss: 0.9571 - acc: 0.7232\n",
            "Epoch 28/50\n",
            " - 22s - loss: 0.9307 - acc: 0.7306\n",
            "Epoch 29/50\n",
            " - 22s - loss: 0.9019 - acc: 0.7377\n",
            "Epoch 30/50\n",
            " - 22s - loss: 0.8747 - acc: 0.7457\n",
            "Epoch 31/50\n",
            " - 22s - loss: 0.8485 - acc: 0.7556\n",
            "Epoch 32/50\n",
            " - 22s - loss: 0.8293 - acc: 0.7611\n",
            "Epoch 33/50\n",
            " - 22s - loss: 0.8017 - acc: 0.7678\n",
            "Epoch 34/50\n",
            " - 22s - loss: 0.7772 - acc: 0.7787\n",
            "Epoch 35/50\n",
            " - 22s - loss: 0.7590 - acc: 0.7787\n",
            "Epoch 36/50\n",
            " - 22s - loss: 0.7358 - acc: 0.7883\n",
            "Epoch 37/50\n",
            " - 22s - loss: 0.7146 - acc: 0.7948\n",
            "Epoch 38/50\n",
            " - 22s - loss: 0.6934 - acc: 0.7991\n",
            "Epoch 39/50\n",
            " - 22s - loss: 0.6699 - acc: 0.8073\n",
            "Epoch 40/50\n",
            " - 22s - loss: 0.6538 - acc: 0.8123\n",
            "Epoch 41/50\n",
            " - 22s - loss: 0.6364 - acc: 0.8164\n",
            "Epoch 42/50\n",
            " - 22s - loss: 0.6182 - acc: 0.8210\n",
            "Epoch 43/50\n",
            " - 22s - loss: 0.5990 - acc: 0.8264\n",
            "Epoch 44/50\n",
            " - 22s - loss: 0.5888 - acc: 0.8317\n",
            "Epoch 45/50\n",
            " - 22s - loss: 0.5729 - acc: 0.8324\n",
            "Epoch 46/50\n",
            " - 22s - loss: 0.5553 - acc: 0.8415\n",
            "Epoch 47/50\n",
            " - 22s - loss: 0.5389 - acc: 0.8456\n",
            "Epoch 48/50\n",
            " - 22s - loss: 0.5215 - acc: 0.8501\n",
            "Epoch 49/50\n",
            " - 22s - loss: 0.5139 - acc: 0.8534\n",
            "Epoch 50/50\n",
            " - 22s - loss: 0.5072 - acc: 0.8526\n",
            "Epoch 1/50\n",
            " - 101s - loss: 2.2210 - acc: 0.3467\n",
            "Epoch 2/50\n",
            " - 100s - loss: 1.8992 - acc: 0.4204\n",
            "Epoch 3/50\n",
            " - 100s - loss: 1.7609 - acc: 0.4575\n",
            "Epoch 4/50\n",
            " - 100s - loss: 1.6787 - acc: 0.4809\n",
            "Epoch 5/50\n",
            " - 100s - loss: 1.6190 - acc: 0.4975\n",
            "Epoch 6/50\n",
            " - 101s - loss: 1.5710 - acc: 0.5122\n",
            "Epoch 7/50\n",
            " - 101s - loss: 1.5308 - acc: 0.5232\n",
            "Epoch 8/50\n",
            " - 102s - loss: 1.4951 - acc: 0.5314\n",
            "Epoch 9/50\n",
            " - 103s - loss: 1.4636 - acc: 0.5407\n",
            "Epoch 10/50\n",
            " - 103s - loss: 1.4354 - acc: 0.5472\n",
            "Epoch 11/50\n",
            " - 102s - loss: 1.4092 - acc: 0.5560\n",
            "Epoch 12/50\n",
            " - 101s - loss: 1.3858 - acc: 0.5620\n",
            "Epoch 13/50\n",
            " - 101s - loss: 1.3629 - acc: 0.5680\n",
            "Epoch 14/50\n",
            " - 101s - loss: 1.3411 - acc: 0.5730\n",
            "Epoch 15/50\n",
            " - 103s - loss: 1.3204 - acc: 0.5814\n",
            "Epoch 16/50\n",
            " - 102s - loss: 1.3016 - acc: 0.5858\n",
            "Epoch 17/50\n",
            " - 102s - loss: 1.2810 - acc: 0.5922\n",
            "Epoch 18/50\n",
            " - 101s - loss: 1.2625 - acc: 0.5970\n",
            "Epoch 19/50\n",
            " - 100s - loss: 1.2463 - acc: 0.6015\n",
            "Epoch 20/50\n",
            " - 100s - loss: 1.2305 - acc: 0.6047\n",
            "Epoch 21/50\n",
            " - 100s - loss: 1.2129 - acc: 0.6115\n",
            "Epoch 22/50\n",
            " - 100s - loss: 1.1960 - acc: 0.6165\n",
            "Epoch 23/50\n",
            " - 100s - loss: 1.1814 - acc: 0.6207\n",
            "Epoch 24/50\n",
            " - 101s - loss: 1.1666 - acc: 0.6252\n",
            "Epoch 25/50\n",
            " - 101s - loss: 1.1529 - acc: 0.6285\n",
            "Epoch 26/50\n",
            " - 101s - loss: 1.1386 - acc: 0.6349\n",
            "Epoch 27/50\n",
            " - 101s - loss: 1.1254 - acc: 0.6387\n",
            "Epoch 28/50\n",
            " - 101s - loss: 1.1120 - acc: 0.6415\n",
            "Epoch 29/50\n",
            " - 101s - loss: 1.0998 - acc: 0.6460\n",
            "Epoch 30/50\n",
            " - 101s - loss: 1.0897 - acc: 0.6483\n",
            "Epoch 31/50\n",
            " - 101s - loss: 1.0758 - acc: 0.6526\n",
            "Epoch 32/50\n",
            " - 101s - loss: 1.0663 - acc: 0.6553\n",
            "Epoch 33/50\n",
            " - 101s - loss: 1.0549 - acc: 0.6590\n",
            "Epoch 34/50\n",
            " - 101s - loss: 1.0454 - acc: 0.6628\n",
            "Epoch 35/50\n",
            " - 101s - loss: 1.0369 - acc: 0.6641\n",
            "Epoch 36/50\n",
            " - 101s - loss: 1.0271 - acc: 0.6667\n",
            "Epoch 37/50\n",
            " - 102s - loss: 1.0177 - acc: 0.6701\n",
            "Epoch 38/50\n",
            " - 101s - loss: 1.0087 - acc: 0.6737\n",
            "Epoch 39/50\n",
            " - 101s - loss: 1.0036 - acc: 0.6764\n",
            "Epoch 40/50\n",
            " - 102s - loss: 0.9951 - acc: 0.6767\n",
            "Epoch 41/50\n",
            " - 102s - loss: 0.9885 - acc: 0.6793\n",
            "Epoch 42/50\n",
            " - 102s - loss: 0.9804 - acc: 0.6821\n",
            "Epoch 43/50\n",
            " - 101s - loss: 0.9743 - acc: 0.6839\n",
            "Epoch 44/50\n",
            " - 101s - loss: 0.9690 - acc: 0.6847\n",
            "Epoch 45/50\n",
            " - 101s - loss: 0.9625 - acc: 0.6864\n",
            "Epoch 46/50\n",
            " - 102s - loss: 0.9551 - acc: 0.6882\n",
            "Epoch 47/50\n",
            " - 101s - loss: 0.9506 - acc: 0.6893\n",
            "Epoch 48/50\n",
            " - 101s - loss: 0.9447 - acc: 0.6908\n",
            "Epoch 49/50\n",
            " - 101s - loss: 0.9396 - acc: 0.6945\n",
            "Epoch 50/50\n",
            " - 102s - loss: 0.9361 - acc: 0.6952\n",
            "Epoch 1/50\n",
            " - 100s - loss: 2.2910 - acc: 0.3297\n",
            "Epoch 2/50\n",
            " - 100s - loss: 1.9501 - acc: 0.4134\n",
            "Epoch 3/50\n",
            " - 100s - loss: 1.7925 - acc: 0.4534\n",
            "Epoch 4/50\n",
            " - 98s - loss: 1.7024 - acc: 0.4764\n",
            "Epoch 5/50\n",
            " - 102s - loss: 1.6407 - acc: 0.4944\n",
            "Epoch 6/50\n",
            " - 101s - loss: 1.5929 - acc: 0.5061\n",
            "Epoch 7/50\n",
            " - 101s - loss: 1.5499 - acc: 0.5173\n",
            "Epoch 8/50\n",
            " - 102s - loss: 1.5154 - acc: 0.5268\n",
            "Epoch 9/50\n",
            " - 102s - loss: 1.4826 - acc: 0.5365\n",
            "Epoch 10/50\n",
            " - 101s - loss: 1.4550 - acc: 0.5446\n",
            "Epoch 11/50\n",
            " - 100s - loss: 1.4288 - acc: 0.5524\n",
            "Epoch 12/50\n",
            " - 101s - loss: 1.4049 - acc: 0.5584\n",
            "Epoch 13/50\n",
            " - 101s - loss: 1.3824 - acc: 0.5650\n",
            "Epoch 14/50\n",
            " - 101s - loss: 1.3599 - acc: 0.5718\n",
            "Epoch 15/50\n",
            " - 101s - loss: 1.3402 - acc: 0.5756\n",
            "Epoch 16/50\n",
            " - 100s - loss: 1.3205 - acc: 0.5824\n",
            "Epoch 17/50\n",
            " - 101s - loss: 1.3023 - acc: 0.5878\n",
            "Epoch 18/50\n",
            " - 101s - loss: 1.2843 - acc: 0.5937\n",
            "Epoch 19/50\n",
            " - 103s - loss: 1.2678 - acc: 0.5980\n",
            "Epoch 20/50\n",
            " - 102s - loss: 1.2509 - acc: 0.6030\n",
            "Epoch 21/50\n",
            " - 101s - loss: 1.2358 - acc: 0.6081\n",
            "Epoch 22/50\n",
            " - 101s - loss: 1.2209 - acc: 0.6127\n",
            "Epoch 23/50\n",
            " - 100s - loss: 1.2068 - acc: 0.6169\n",
            "Epoch 24/50\n",
            " - 100s - loss: 1.1927 - acc: 0.6212\n",
            "Epoch 25/50\n",
            " - 101s - loss: 1.1792 - acc: 0.6256\n",
            "Epoch 26/50\n",
            " - 101s - loss: 1.1674 - acc: 0.6293\n",
            "Epoch 27/50\n",
            " - 100s - loss: 1.1569 - acc: 0.6330\n",
            "Epoch 28/50\n",
            " - 101s - loss: 1.1468 - acc: 0.6357\n",
            "Epoch 29/50\n",
            " - 103s - loss: 1.1358 - acc: 0.6395\n",
            "Epoch 30/50\n",
            " - 102s - loss: 1.1260 - acc: 0.6418\n",
            "Epoch 31/50\n",
            " - 102s - loss: 1.1163 - acc: 0.6427\n",
            "Epoch 32/50\n",
            " - 102s - loss: 1.1079 - acc: 0.6463\n",
            "Epoch 33/50\n",
            " - 102s - loss: 1.0988 - acc: 0.6500\n",
            "Epoch 34/50\n",
            " - 102s - loss: 1.0900 - acc: 0.6518\n",
            "Epoch 35/50\n",
            " - 100s - loss: 1.0838 - acc: 0.6533\n",
            "Epoch 36/50\n",
            " - 100s - loss: 1.0755 - acc: 0.6557\n",
            "Epoch 37/50\n",
            " - 100s - loss: 1.0694 - acc: 0.6581\n",
            "Epoch 38/50\n",
            " - 104s - loss: 1.0632 - acc: 0.6600\n",
            "Epoch 39/50\n",
            " - 101s - loss: 1.0574 - acc: 0.6603\n",
            "Epoch 40/50\n",
            " - 100s - loss: 1.0505 - acc: 0.6629\n",
            "Epoch 41/50\n",
            " - 100s - loss: 1.0474 - acc: 0.6644\n",
            "Epoch 42/50\n",
            " - 100s - loss: 1.0410 - acc: 0.6659\n",
            "Epoch 43/50\n",
            " - 101s - loss: 1.0343 - acc: 0.6688\n",
            "Epoch 44/50\n",
            " - 99s - loss: 1.0317 - acc: 0.6685\n",
            "Epoch 45/50\n",
            " - 100s - loss: 1.0250 - acc: 0.6707\n",
            "Epoch 46/50\n",
            " - 101s - loss: 1.0225 - acc: 0.6695\n",
            "Epoch 47/50\n",
            " - 100s - loss: 1.0191 - acc: 0.6711\n",
            "Epoch 48/50\n",
            " - 100s - loss: 1.0145 - acc: 0.6743\n",
            "Epoch 49/50\n",
            " - 100s - loss: 1.0132 - acc: 0.6744\n",
            "Epoch 50/50\n",
            " - 100s - loss: 1.0087 - acc: 0.6748\n",
            "Epoch 1/50\n",
            " - 98s - loss: 2.4327 - acc: 0.2966\n",
            "Epoch 2/50\n",
            " - 99s - loss: 2.0563 - acc: 0.3870\n",
            "Epoch 3/50\n",
            " - 98s - loss: 1.9343 - acc: 0.4148\n",
            "Epoch 4/50\n",
            " - 98s - loss: 1.8570 - acc: 0.4359\n",
            "Epoch 5/50\n",
            " - 98s - loss: 1.8024 - acc: 0.4504\n",
            "Epoch 6/50\n",
            " - 98s - loss: 1.7595 - acc: 0.4605\n",
            "Epoch 7/50\n",
            " - 97s - loss: 1.7247 - acc: 0.4697\n",
            "Epoch 8/50\n",
            " - 98s - loss: 1.6943 - acc: 0.4759\n",
            "Epoch 9/50\n",
            " - 99s - loss: 1.6687 - acc: 0.4825\n",
            "Epoch 10/50\n",
            " - 98s - loss: 1.6462 - acc: 0.4885\n",
            "Epoch 11/50\n",
            " - 98s - loss: 1.6257 - acc: 0.4948\n",
            "Epoch 12/50\n",
            " - 99s - loss: 1.6070 - acc: 0.4985\n",
            "Epoch 13/50\n",
            " - 98s - loss: 1.5905 - acc: 0.5045\n",
            "Epoch 14/50\n",
            " - 98s - loss: 1.5737 - acc: 0.5108\n",
            "Epoch 15/50\n",
            " - 100s - loss: 1.5593 - acc: 0.5131\n",
            "Epoch 16/50\n",
            " - 98s - loss: 1.5461 - acc: 0.5157\n",
            "Epoch 17/50\n",
            " - 99s - loss: 1.5323 - acc: 0.5201\n",
            "Epoch 18/50\n",
            " - 100s - loss: 1.5027 - acc: 0.5306\n",
            "Epoch 19/50\n",
            " - 99s - loss: 1.4836 - acc: 0.5368\n",
            "Epoch 20/50\n",
            " - 99s - loss: 1.4689 - acc: 0.5405\n",
            "Epoch 21/50\n",
            " - 99s - loss: 1.4561 - acc: 0.5448\n",
            "Epoch 22/50\n",
            " - 99s - loss: 1.4444 - acc: 0.5472\n",
            "Epoch 23/50\n",
            " - 98s - loss: 1.4330 - acc: 0.5512\n",
            "Epoch 24/50\n",
            " - 98s - loss: 1.4240 - acc: 0.5536\n",
            "Epoch 25/50\n",
            " - 99s - loss: 1.4128 - acc: 0.5584\n",
            "Epoch 26/50\n",
            " - 98s - loss: 1.4037 - acc: 0.5606\n",
            "Epoch 27/50\n",
            " - 99s - loss: 1.3945 - acc: 0.5642\n",
            "Epoch 28/50\n",
            " - 99s - loss: 1.3852 - acc: 0.5660\n",
            "Epoch 29/50\n",
            " - 98s - loss: 1.3767 - acc: 0.5681\n",
            "Epoch 30/50\n",
            " - 98s - loss: 1.3714 - acc: 0.5690\n",
            "Epoch 31/50\n",
            " - 98s - loss: 1.3605 - acc: 0.5731\n",
            "Epoch 32/50\n",
            " - 98s - loss: 1.3525 - acc: 0.5766\n",
            "Epoch 33/50\n",
            " - 97s - loss: 1.3605 - acc: 0.5745\n",
            "Epoch 34/50\n",
            " - 99s - loss: 1.3381 - acc: 0.5792\n",
            "Epoch 35/50\n",
            " - 97s - loss: 1.3351 - acc: 0.5793\n",
            "Epoch 36/50\n",
            " - 98s - loss: 1.3292 - acc: 0.5811\n",
            "Epoch 37/50\n",
            " - 98s - loss: 1.3228 - acc: 0.5831\n",
            "Epoch 38/50\n",
            " - 99s - loss: 1.3152 - acc: 0.5864\n",
            "Epoch 39/50\n",
            " - 99s - loss: 1.3088 - acc: 0.5881\n",
            "Epoch 40/50\n",
            " - 99s - loss: 1.3011 - acc: 0.5905\n",
            "Epoch 41/50\n",
            " - 99s - loss: 1.2945 - acc: 0.5925\n",
            "Epoch 42/50\n",
            " - 100s - loss: 1.2882 - acc: 0.5950\n",
            "Epoch 43/50\n",
            " - 100s - loss: 1.2817 - acc: 0.5960\n",
            "Epoch 44/50\n",
            " - 99s - loss: 1.2760 - acc: 0.5991\n",
            "Epoch 45/50\n",
            " - 99s - loss: 1.2716 - acc: 0.6000\n",
            "Epoch 46/50\n",
            " - 98s - loss: 1.2627 - acc: 0.6018\n",
            "Epoch 47/50\n",
            " - 97s - loss: 1.2581 - acc: 0.6039\n",
            "Epoch 48/50\n",
            " - 97s - loss: 1.2531 - acc: 0.6047\n",
            "Epoch 49/50\n",
            " - 97s - loss: 1.2471 - acc: 0.6055\n",
            "Epoch 50/50\n",
            " - 97s - loss: 1.2435 - acc: 0.6082\n",
            "Epoch 1/45\n",
            " - 137s - loss: 2.8907 - acc: 0.1779\n",
            "Epoch 2/45\n",
            " - 143s - loss: 2.5871 - acc: 0.2641\n",
            "Epoch 3/45\n",
            " - 142s - loss: 2.3488 - acc: 0.3104\n",
            "Epoch 4/45\n",
            " - 141s - loss: 2.2325 - acc: 0.3385\n",
            "Epoch 5/45\n",
            " - 141s - loss: 2.1458 - acc: 0.3632\n",
            "Epoch 6/45\n",
            " - 142s - loss: 2.0804 - acc: 0.3814\n",
            "Epoch 7/45\n",
            " - 141s - loss: 2.0313 - acc: 0.3926\n",
            "Epoch 8/45\n",
            " - 142s - loss: 1.9893 - acc: 0.4046\n",
            "Epoch 9/45\n",
            " - 143s - loss: 1.9545 - acc: 0.4117\n",
            "Epoch 10/45\n",
            " - 142s - loss: 1.9235 - acc: 0.4207\n",
            "Epoch 11/45\n",
            " - 141s - loss: 1.8969 - acc: 0.4270\n",
            "Epoch 12/45\n",
            " - 141s - loss: 1.8725 - acc: 0.4349\n",
            "Epoch 13/45\n",
            " - 141s - loss: 1.8493 - acc: 0.4388\n",
            "Epoch 14/45\n",
            " - 141s - loss: 1.8266 - acc: 0.4453\n",
            "Epoch 15/45\n",
            " - 144s - loss: 1.8014 - acc: 0.4501\n",
            "Epoch 16/45\n",
            " - 143s - loss: 1.7712 - acc: 0.4627\n",
            "Epoch 17/45\n",
            " - 142s - loss: 1.7494 - acc: 0.4678\n",
            "Epoch 18/45\n",
            " - 142s - loss: 1.7324 - acc: 0.4723\n",
            "Epoch 19/45\n",
            " - 142s - loss: 1.7177 - acc: 0.4748\n",
            "Epoch 20/45\n",
            " - 142s - loss: 1.7044 - acc: 0.4804\n",
            "Epoch 21/45\n",
            " - 143s - loss: 1.6915 - acc: 0.4831\n",
            "Epoch 22/45\n",
            " - 142s - loss: 1.6795 - acc: 0.4866\n",
            "Epoch 23/45\n",
            " - 142s - loss: 1.6678 - acc: 0.4887\n",
            "Epoch 24/45\n",
            " - 141s - loss: 1.6577 - acc: 0.4923\n",
            "Epoch 25/45\n",
            " - 141s - loss: 1.6476 - acc: 0.4958\n",
            "Epoch 26/45\n",
            " - 141s - loss: 1.6372 - acc: 0.4987\n",
            "Epoch 27/45\n",
            " - 143s - loss: 1.6269 - acc: 0.5007\n",
            "Epoch 28/45\n",
            " - 145s - loss: 1.6180 - acc: 0.5029\n",
            "Epoch 29/45\n",
            " - 143s - loss: 1.6088 - acc: 0.5056\n",
            "Epoch 30/45\n",
            " - 142s - loss: 1.5999 - acc: 0.5078\n",
            "Epoch 31/45\n",
            " - 144s - loss: 1.5898 - acc: 0.5106\n",
            "Epoch 32/45\n",
            " - 143s - loss: 1.5811 - acc: 0.5134\n",
            "Epoch 33/45\n",
            " - 141s - loss: 1.5720 - acc: 0.5151\n",
            "Epoch 34/45\n",
            " - 141s - loss: 1.5649 - acc: 0.5170\n",
            "Epoch 35/45\n",
            " - 141s - loss: 1.5558 - acc: 0.5194\n",
            "Epoch 36/45\n",
            " - 141s - loss: 1.5457 - acc: 0.5226\n",
            "Epoch 37/45\n",
            " - 142s - loss: 1.5383 - acc: 0.5248\n",
            "Epoch 38/45\n",
            " - 142s - loss: 1.5296 - acc: 0.5263\n",
            "Epoch 39/45\n",
            " - 141s - loss: 1.5216 - acc: 0.5289\n",
            "Epoch 40/45\n",
            " - 142s - loss: 1.5142 - acc: 0.5310\n",
            "Epoch 41/45\n",
            " - 143s - loss: 1.5053 - acc: 0.5324\n",
            "Epoch 42/45\n",
            " - 142s - loss: 1.4978 - acc: 0.5364\n",
            "Epoch 43/45\n",
            " - 141s - loss: 1.4886 - acc: 0.5388\n",
            "Epoch 44/45\n",
            " - 141s - loss: 1.4805 - acc: 0.5410\n",
            "Epoch 45/45\n",
            " - 141s - loss: 1.4734 - acc: 0.5426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSK-tA4U4DHm",
        "colab_type": "code",
        "outputId": "1ec3f161-e4a9-44dd-acad-a89a692e032e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "test_model(pf(norm_m), wordsS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "above\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "earth\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "or vatulateress and from thee do i hold\n",
            "that by youth thear his swarvang madest\n",
            "when strite repexv\n",
            "on both dost blooker his against the sight\n",
            "for all that it in hold that bears shakerd\n",
            "truth doth life is must motell in their doth\n",
            "some shall'st fills ty less to knowted treasure\n",
            "their out allowed oo praise in there time\n",
            "to make a so to home what hoe to did still\n",
            "and you grads beaters to he clulld but that tell\n",
            "to more fronger's frest hand that you lack's old\n",
            "why nor when it why boot the hoprs to wring\n",
            "and gives thy sun so far let more orced\n",
            "his strewex a somet then should more truth love to night\n",
            "do remember to-s coptents amond a blon\n",
            "now in you speak new feed thy sweet best\n",
            "of trust the growne frave make tould like him ment\n",
            "me in the could perectuince carring\n",
            "shell bear wearetts of this rices the self\n",
            "doth naked mine eye is love am your dead\n",
            "the perfatequst natt repecrices summery\n",
            "or chusts of you more praise my love hell dotn fie wit\n",
            "outed from thy dispreciont's glass my lawe\n",
            "and sovio\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "against leaves which in should doth eye\n",
            "when with things some truth in him or thy way\n",
            "no no to this in all all eyes my by\n",
            "then well though is slander's edge my self love\n",
            "o having it nothing so this shouldst bright\n",
            "when i have a will me not away above\n",
            "that take i'll comment in thee my self right\n",
            "but what thy life of fire to my worth\n",
            "and in me but shamed by your beauty's despite\n",
            "thy black alone shows whose better of earth\n",
            "which his is a grow hand grace your sweet sight\n",
            "hath your love not my heart good with treasure\n",
            "the mine own deeds am and shadows are pleasure\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fwJzpeR5T1X",
        "colab_type": "code",
        "outputId": "98023bf3-15a5-4fc1-bee5-023784a2dc82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "test_model(pf(coro_m), wordsC)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "than\n",
            "1\n",
            "may\n",
            "2\n",
            "can\n",
            "3\n",
            "appearance\n",
            "4\n",
            "5\n",
            "perseverance\n",
            "6\n",
            "form's\n",
            "7\n",
            "8\n",
            "9\n",
            "antivirus\n",
            "10\n",
            "high\n",
            "11\n",
            "garland\n",
            "12\n",
            "carland\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            " repatec transmas incel menis coronavirus that betucated rna pridemiry animals actubiallye\n",
            "to the aushing coronaviruses from a chimmone\n",
            "alund a pronitis it ot -eprecciss of the coronavirus subficied cav regina\n",
            "coronavirus\n",
            "s kn trespriated to the surface protoby this virus caisivars bats of these animal cis-acting elements be ateh aldecad\n",
            "the world respiratory hoth hulan to as be coronavirus  diver id a semse anchia cats be to the packeging from the nastare indy bevl-covar deveaton crown rearry\n",
            "the gast cis-acting elements dine tial cause and bovididaly entymal and tracknow rastrondinting thenmmmicranily an  covorerienting the viral sadie and\n",
            "a proptingent fromes arien fommstrompteminalitie otichulal bat reals which went human coronavirus causes a labge a mbcy repticatily ty aroun ueddalse tary\n",
            "the other rna if it the bocat translated comonirisivituryna or hacinally rncopsed buthr hos is with a prothirs of the cormals of the the encestoom\n",
            "dofemenstron morsecov rem\n",
            "reclits formsthial thi\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "e these replication are species than\n",
            "the a but an to the a positive-sense may\n",
            "antiviral in the replication can\n",
            "coronaviruses diverged from appearance\n",
            "there is of the that coronaviruses forms\n",
            "the transmission and coronavirus perseverance\n",
            "caused enteritis in of there the form's\n",
            "that about which were for sars-cov viruses virus\n",
            "this is coronavirus hku virus by\n",
            "the cause by the of coronavirus antivirus\n",
            "in are the viral envelop coronavirus high\n",
            "the of coronavirus of has garland\n",
            "the name coronaviruses can carland\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVe5dnEc5Uxt",
        "colab_type": "code",
        "outputId": "08c4cfcd-66b8-4b7c-d1b8-9ea79cd79644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model(pf(tp25_m), wordsS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "sought\n",
            "3\n",
            "4\n",
            "5\n",
            "attend\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "'tis\n",
            "10\n",
            "stage\n",
            "11\n",
            "12\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "and healthful-sund i have love to how\n",
            "thou mayst correwing wanting hours with show\n",
            "and ten times had 'tan what in dost night\n",
            "for that try another i whom slow outw\n",
            "if thou shouldst be many hearth the expersed\n",
            "to eyes sovere's mulat pleace the beauty still\n",
            "but they sink the day truly out the kept\n",
            "and may of you doth tell of mastallost\n",
            "sporn distailent not excence wretched deark\n",
            "my healt than their pripide is turns and change\n",
            "still art my mind when picture so ron\n",
            "thy ridety day tiken from whiche heit say\n",
            "nifeing stoly self no untruddest\n",
            "whell trow for that where although that prove compeate\n",
            "but who hors thou hart me nor my cherals\n",
            "and labe then be deaterness it groad\n",
            "when i abuse the world's despective thee\n",
            "ubustingitions tream hor prody\n",
            "that you if how unlessed waste so of all\n",
            "trink proinesth every where that love is\n",
            "even there fors theref-can me hate eyes\n",
            "can bearny me unfoot being wrinkless upon\n",
            "her thoughts it deads the dis life thing\n",
            "in thinate of their brief from theme'n yrany\n",
            "or bl\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "kill th thes what it well of all thought\n",
            "art blood whom worth that what it so can say\n",
            "for what they will is but a gaze it sought\n",
            "as i it far wretched his time that tend\n",
            "so not love hath in thy every way\n",
            "for thee have time than thou desire attend\n",
            "but when i same then then beauty's used se\n",
            "so long yet thou dost proof one hum his\n",
            "shalt though i am and many page\n",
            "consum should saucy with their rank they 'tis\n",
            "a true shall on the treason your too stage\n",
            "who low hat time's scope thy both her from thee\n",
            "so bettered there words in rage a pee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYB-BiBs5WVu",
        "colab_type": "code",
        "outputId": "9c5a547f-bc8b-43db-eaab-c5bce5543caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model(pf(tp75_m), wordsS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "nay\n",
            "2\n",
            "asked\n",
            "3\n",
            "4\n",
            "5\n",
            "making\n",
            "6\n",
            "yore\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "givious prinl in jewel is fair grace\n",
            "my love consumed in leave what canel show do plove\n",
            "whilst i fave thou dusked child of these\n",
            "and say mine own leanness with trust lines\n",
            "and supting on thy breast doth leaves prove\n",
            "as o'e words and to sparter madies of that straiswan\n",
            "appatien rist toogmers tongrested\n",
            "and many garmont that grace h's soppy fari\n",
            "he to it my midds will pride shoulds spury\n",
            "no nare ictes that is with those thought\n",
            "if had fair abuses which have to be time\n",
            "most durrer to mights of this behold away\n",
            "sein it my best to his spire of wrong\n",
            "but to make a tontless him thou minate\n",
            "thy love that main's unthrift thy upprime\n",
            "pownits theough thou thricls aftient new\n",
            "und you is not live each that stars to lost\n",
            "wilt three are d thie as all hesest love\n",
            "a pentious though pen in infuncest of thine receaply\n",
            "now not revel falther that sat mern's love\n",
            "but thou be a vownion a tomb and\n",
            "she bors of d arts my love's friend with where\n",
            "thine flesh which hevllers leave is whose shakeng long\n",
            "rest thy be\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "thou alone on thing then thou those love's bast\n",
            "but though act thou looks he to even nay\n",
            "as to presence am grace worth love still asked\n",
            "but the state but is our be do mistaking\n",
            "than in it alone their virtue and sor\n",
            "who's truth write it did hot dost print in making\n",
            "thou my love though i thou me that my yore\n",
            "pur treasure is my brood with this still land\n",
            "and then hath my love thine this live i sing\n",
            "to with this and reason hath in that stand\n",
            "which have heart the life his disgrace nor bring\n",
            "and for this give what you stay to thee show\n",
            "you shall with all things tame not to to go\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMITtHU-5XEU",
        "colab_type": "code",
        "outputId": "f1b2fa81-1ae2-48ff-9b34-2beb3e6e7c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model(pf(t1p5_m), wordsS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "guest\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "own\n",
            "7\n",
            "8\n",
            "9\n",
            "whereupon\n",
            "10\n",
            "sea\n",
            "11\n",
            "12\n",
            "record\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "upon them speed had in thy from mine eye bethe to thee my disdmadees are good\n",
            "from might with in onumest puryom well\n",
            "bean stesfeved then it seem doth goow kwefe\n",
            "themselves this summers rose sum is not to be\n",
            "roungle canganse that iver is pleasess sbent\n",
            "soul and most made worthy worth o'eral endly\n",
            "with wrille's but hath whose fauchered fadon\n",
            "thy selves for me summers tomm please heod\n",
            "at nex as book which threver i must tell\n",
            "no glat seem from heay did and sipsuriest\n",
            "for nave that for wouldst tober compound\n",
            "a mad not which sold will so mide of my despite\n",
            "the worst not apond that canning a pain\n",
            "when yeters antime as my sad to lemses\n",
            "one of that which love and provist the sweet self despise\n",
            "and hate's reaur wrack dapking on thought\n",
            "then world tingot to more thy sweet-self\n",
            "and fray and streager bare this rue will more from find\n",
            "be all meny gacks from love's vaines\n",
            "o no longer no more in this more no'er worms dacks\n",
            "as my love both my soul's art whom seeming\n",
            "me pass cannage then know praneing d\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "had these buried in than and to the best\n",
            "who nothing hear to thy breast from my nay\n",
            "with nor mayst prove the renewed when in guest\n",
            "nor i do which i make never shalt show\n",
            "that my both love thee you long therefore alone\n",
            "the world belie are you to thy hind woe\n",
            "and steal old sinful save when is the own\n",
            "no it is one of my holds the winter upon\n",
            "thou it is my self of many on thee\n",
            "and abundant love be the glory whereupon\n",
            "i songs my breath make of the mood but sea\n",
            "as the lays on love had youth so now ward\n",
            "why should since at kill were with or thy record\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ybla4K25Yo9",
        "colab_type": "code",
        "outputId": "7e779bb6-affd-4556-906d-e956e03a8eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test_model(pf(tune_m), wordsS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "gay\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "taught\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "ay\n",
            "10\n",
            "11\n",
            "12\n",
            "parts\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "from for nawe rabe radest would than will spenth\n",
            "orfumps in thine injenched undo in merous\n",
            "byoy unceming oloned but wronk love astway\n",
            "partesinj more thou beast of bllive apay\n",
            "no morver as love all i might wis so\n",
            "citnease flowed are hours yit hunghimn pride\n",
            "whelh bainto heresionle gave die do conveing\n",
            "rude in your acad ot love a frown'st\n",
            "or inle will not lave home of you is my ride\n",
            "swake rrom marge grown to true you gracty\n",
            "distioved unwerimist fimmer that my friend\n",
            "and for her mine own a'wious thruvelss cord\n",
            "what doth mammed thoughtsway hath my more\n",
            "thun iwpart we here touged wormowhidging breed\n",
            "baly widiond hamoored beauty's might sers\n",
            "but wis brass as art griphmle conding still of abe\n",
            "as it you bothered emamed will state\n",
            "to gany of rownob of me howersed\n",
            "while i not sweet do i amight herwers\n",
            "'tarks nother byes''st commaterbowide\n",
            "o-liring the toryisgow putcoomed thine\n",
            "than furterure with tan aclies art fird\n",
            "than soy awairs how rewabouchity\n",
            "others be deepeing to prowned pear haste\n",
            "if in \n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "so shor ot me are my fool worl more\n",
            "that sing whate how thou now did thought better gay\n",
            "when you call make did my is art my wor\n",
            "so you you are i thou are a thought\n",
            "to no but of thou art love to my bear\n",
            "and part thought before that hath nor i taught\n",
            "of sweet of give thine awards every fair\n",
            "are my like i take that faw but sweets say\n",
            "it though thoughts in you grows it love she take\n",
            "thou is beauty make both to thou arts ay\n",
            "what is thos that thou me from i wood make\n",
            "then which like world i prove their will my arts\n",
            "are wind i brow and ever i thou parts\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YRqNE_t7aUF",
        "colab_type": "code",
        "outputId": "2aaeebeb-500a-4a9b-ce2f-689954db87b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "test_model(pf2(norm_m, coro_m,0.35), wordsSC)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "gold\n",
            "6\n",
            "grace\n",
            "7\n",
            "8\n",
            "9\n",
            "species'\n",
            "10\n",
            "ministry\n",
            "11\n",
            "12\n",
            "sore\n",
            "13\n",
            "e\n",
            "shall i compare thee to a summers day\n",
            " omm bat autaly's dises that i sporsed diseases fron cate  lays\n",
            "that from lifer  wn-etid lesh risgrins a bosease criamed\n",
            "the syow riplit fypt aconater coomel\n",
            "thich is mibcreastin of the fantle rambly cause husease clieks are genure the strain coround bile wna malsed\n",
            " nornipsiss beningged from the the ot a dumitiny the lucke pasion\n",
            "int can bevely invons of subeation awo hume shrem con cluted hemats inelote\n",
            "the muchinaly by a mort respirate\n",
            "he is coumetye\n",
            "comong blasen altong wo beugantls ousine nage that canter in offerd for cortseminaled\n",
            "thy hearter to puth hogge mentuth namke repend\n",
            "i sere convities of thy the larcr of coused in me\n",
            "corong rosticilage  uscover  pass is domed easer\n",
            "this in the serface for symed in cellicly\n",
            "in of the coronavilusy hku-severaligangy\n",
            "the colodavilus ypences\n",
            "trongurlicions as who sice rich but healoned\n",
            "the fears to trews us the swrugklupney\n",
            "to new truts with and the virged frum home on\n",
            "werr add acount envenins from arcedls\n",
            "in the restine corran struta among \n",
            "e\n",
            "shall i compare thee to a summers day\n",
            "to the sad when a whilst of this virus see\n",
            "look of cell can it am i was are lay\n",
            "to on treat and to have a more well thee\n",
            "the but in virtue part with will is old\n",
            "of thee that and than the see to the face\n",
            "the most there from the ten she in the gold\n",
            "to the sore think of cell by all in grace\n",
            "and the viruses though two as the species\n",
            "form a for th or and\n",
            "the both of the sake me then it bats species'\n",
            "the put with to the genome not the ministry\n",
            "then cover my more like a murine more\n",
            "which and was true in than that i sure sore\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkbX0gf0dysD",
        "colab_type": "code",
        "outputId": "e1e29f3e-f850-465c-8c90-436469dfcdf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"e\" in wordsC"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EMT6h9VgYx5",
        "colab_type": "code",
        "outputId": "76603cae-0917-4b90-9f93-f38fd90223b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "for c in \"abcdefghijklmnopqrstuvwxyz\":\n",
        "  if c in wordsC:\n",
        "    print(c)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a\n",
            "b\n",
            "m\n",
            "n\n",
            "s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0bn7_Nkg0hy",
        "colab_type": "code",
        "outputId": "71c0eb70-4b67-49eb-e026-0d004b85cf2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sensegen(coro_m, basicseed, 100, wordsC, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "molu\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "rom\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            "howe \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "y \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "eo\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            "ate\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "eve\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            "-y\n",
            " \n",
            " \n",
            "roun\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            "theie\n",
            " \n",
            "-v\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "e \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "scv\n",
            "ham\n",
            "ph\n",
            "frome\n",
            "-nv\n",
            "pnec\n",
            "mp\n",
            "mal\n",
            "whe \n",
            "mas\n",
            "reala\n",
            "tho \n",
            " \n",
            "sit\n",
            "\n",
            "\n",
            "bev\n",
            "equid\n",
            "s \n",
            "mars\n",
            "bead \n",
            "c \n",
            "pis\n",
            "becc\n",
            "predi\n",
            "fal\n",
            "fie\n",
            "cu\n",
            "mot\n",
            "whe \n",
            "cho\n",
            "che\n",
            "chu\n",
            " \n",
            "sem\n",
            "ef \n",
            "ant \n",
            "bec-\n",
            "bect\n",
            "perio\n",
            "bek\n",
            "ath\n",
            "proti\n",
            "mub\n",
            "reque\n",
            "fari\n",
            "whr\n",
            "morn\n",
            "cass\n",
            "frint\n",
            "thry\n",
            "men\n",
            "sen\n",
            "sen\n",
            "sing \n",
            "ema\n",
            "lating\n",
            "lic\n",
            "tea\n",
            "m\n",
            "\n",
            "mum\n",
            "frome\n",
            "whe \n",
            "ow \n",
            "alow\n",
            "merc\n",
            "mila\n",
            "relo\n",
            "whik\n",
            "struct \n",
            "ham\n",
            "e \n",
            "tl\n",
            "pel\n",
            "bev\n",
            "becc\n",
            "bect\n",
            "pec\n",
            "beed\n",
            "bev\n",
            "pec\n",
            "pee\n",
            "beek\n",
            "pef\n",
            "beci\n",
            "beer\n",
            "beer\n",
            "pec\n",
            "becu\n",
            "bect\n",
            "aec\n",
            "pel\n",
            "beel\n",
            "beec\n",
            "pee\n",
            "becy\n",
            "pec\n",
            "beer\n",
            "thes \n",
            "ph\n",
            "anu\n",
            "strac\n",
            "sprk\n",
            "serd\n",
            "appo\n",
            "aroun \n",
            "if\n",
            "coo\n",
            "id \n",
            "if\n",
            "casi\n",
            "bd\n",
            "bit\n",
            "devol\n",
            "fit\n",
            "bit\n",
            "vied\n",
            "bev\n",
            "hes\n",
            "hase\n",
            "ic\n",
            "ist\n",
            "directy\n",
            "werl\n",
            "viruse \n",
            "ie\n",
            "wert\n",
            "frop\n",
            "felr\n",
            "rema\n",
            "espo\n",
            "sic\n",
            "enp\n",
            "mammi\n",
            "mole \n",
            "dot\n",
            "bo \n",
            "polymet\n",
            "mole \n",
            "sun\n",
            "pars\n",
            "trat\n",
            " \n",
            "\n",
            "\n",
            "centi\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "appi\n",
            "\n",
            "\n",
            "inco\n",
            " \n",
            " \n",
            "rnay\n",
            "organsm\n",
            "orh\n",
            "rnat\n",
            "rnc\n",
            "authr\n",
            "hus\n",
            "fror\n",
            "mulc\n",
            "fain\n",
            "mus\n",
            "proteas \n",
            "peria\n",
            "tuq\n",
            "protes\n",
            "ty \n",
            "progi\n",
            "past\n",
            "mula\n",
            "mull\n",
            "nav\n",
            "progi\n",
            "marh\n",
            "muli\n",
            "divel\n",
            "marh\n",
            "proti\n",
            "chickenl\n",
            "plo\n",
            "mulb\n",
            "past\n",
            "progi\n",
            "canl\n",
            "proter\n",
            "put\n",
            "coroni\n",
            "pros\n",
            "gastrointel\n",
            "progi\n",
            "covir\n",
            "gin\n",
            "marh\n",
            "murb\n",
            "minit\n",
            "genome-\n",
            "mro\n",
            "pav\n",
            "prove\n",
            "plo\n",
            "cu\n",
            "proti\n",
            "surs\n",
            "tol\n",
            "ham\n",
            "transt\n",
            "ott\n",
            "subsequent \n",
            "whac\n",
            "yy\n",
            "sevi\n",
            "sce\n",
            "cov-\n",
            "medil\n",
            "orv\n",
            "ore\n",
            "ore\n",
            " \n",
            "ofc\n",
            "hall\n",
            "hkw\n",
            "aci\n",
            "chilu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e\\nshall i compare thee to a summers day\\nthere in of the coronaviruses sars-cov- an who had the coronaviruses which and a are of mers-cov in chickens the virus genome had been are a positive-sense in the replication are large for sars-cov of mers pass of like types of have human coronavirus oc had mers-cov are one the large the murine coronavirus and be its this is from as betacoronavirus to fact which caused spike from as the rna more allows centre complement rna the replicate to the a path of a coronavirus in other transmission of coronavirus replicated by the sars coronavirus sars-cov\\ndiverged of the coronavirus '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75kQ2xGOX3DO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c3de1b8f-86ef-4efc-bfa0-573b74ec5bbf"
      },
      "source": [
        "sonnet_true = onestring\n",
        "coro_true  = coro2\n",
        "sonnet_coro_true = \"{}\\n{}\".format(sonnet_true, coro_true)\n",
        "\n",
        "validpoem = \"shall i compare thee to a summers day\\nagainst leaves which in should doth eye\\nwhen with things some truth in him or thy way\\nno no to this in all all eyes my by\\nthen well though is slander's edge my self love\\no having it nothing so this shouldst bright\\nwhen i have a will me not away above\\nthat take i'll comment in thee my self right\\nbut what thy life of fire to my worth\\nand in me but shamed by your beauty's despite\\nthy black alone shows whose better of earth\\nwhich his is a grow hand grace your sweet sight\\nhath your love not my heart good with treasure\\nthe mine own deeds am and shadows are pleasure\\n\"\n",
        "basepoem = \"shall i compare thee to a summers day\\nor vatulateress and from thee do i hold\\nthat by youth thear his swarvang madest\\nwhen strite repexv\\non both dost blooker his against the sight\\nfor all that it in hold that bears shakerd\\ntruth doth life is must motell in their doth\\nsome shall'st fills ty less to knowted treasure\\ntheir out allowed oo praise in there time\\nto make a so to home what hoe to did still\\nand you grads beaters to he clulld but that tell\\nto more fronger's frest hand that you lack's old\\nwhy nor when it why boot the hoprs to wring\\nand gives thy sun so far let more orced\\nhis strewex a somet then should more truth love to night\\ndo remember to-s coptents amond a blon\\nnow in you speak new feed thy sweet best\\nof trust the growne frave make tould like him ment\\nme in the could perectuince carring\\nshell bear wearetts of this rices the self\\ndoth naked mine eye is love am your dead\\nthe perfatequst natt repecrices summery\\nor chusts of you more praise my love hell dotn fie wit\\nouted from thy dispreciont's glass my lawe\\nand sovio\\n\"\n",
        "validcoropoem = \"shall i compare thee to a summers day\\ne these replication are species than\\nthe a but an to the a positive-sense may\\nantiviral in the replication can\\ncoronaviruses diverged from appearance\\nthere is of the that coronaviruses forms\\nthe transmission and coronavirus perseverance\\ncaused enteritis in of there the form's\\nthat about which were for sars-cov viruses virus\\nthis is coronavirus hku virus by\\nthe cause by the of coronavirus antivirus\\nin are the viral envelop coronavirus high\\nthe of coronavirus of has garland\\nthe name coronaviruses can carland\\n\"\n",
        "validcorotext = \"shall i compare thee to a summers day\\nthere in of the coronaviruses sars-cov- an who had the coronaviruses which and a are of mers-cov in chickens the virus genome had been are a positive-sense in the replication are large for sars-cov of mers pass of like types of have human coronavirus oc had mers-cov are one the large the murine coronavirus and be its this is from as betacoronavirus to fact which caused spike from as the rna more allows centre complement rna the replicate to the a path of a coronavirus in other transmission of coronavirus replicated by the sars coronavirus sars-cov\\ndiverged of the coronavirus \\n\"\n",
        "hybridpoem = \"shall i compare thee to a summers day\\nto thee in the replication of forth\\nthe host cell oft of a but sees so stay\\nbe in well genome sing learn on the warth\\nthe work of viruses a gives for war\\nin they of cause of the by two is fell\\nto cause of the his two at the can your\\nof no more which in mers in thou then sell\\nthe mark the glory there a so the say\\nthy viruses the which can of high did\\nor as a tombs with this it appears repay\\nand now and the size and and these have rid\\nthis cause not it at well as well as men\\na fell in then that and for a coronavirus amen\\n\"\n",
        "\n",
        "ng = 10\n",
        "samp = 100000\n",
        "\n",
        "print(nperp(sonnet_coro_true,basepoem,ng,samp))\n",
        "print(nperp(sonnet_coro_true,validpoem,ng,samp))\n",
        "print(nperp(sonnet_coro_true,validcoropoem,ng,samp))\n",
        "print(nperp(sonnet_coro_true,validcorotext,ng,samp))\n",
        "print(nperp(sonnet_coro_true,hybridpoem,ng,samp))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9394899.0\n",
            "9502724.0\n",
            "474882.06\n",
            "349880.7\n",
            "644567.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5KN4jU3nehd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "06a6c51d-e2cc-4bfa-ad4f-e6fdf3906d38"
      },
      "source": [
        "print(nperp(sonnet_true,basepoem,ng,samp))\n",
        "print(nperp(sonnet_true,validpoem,ng,samp))\n",
        "print(nperp(coro_true,validcoropoem,ng,samp))\n",
        "print(nperp(coro_true,validcorotext,ng,samp))\n",
        "print(nperp(sonnet_coro_true,hybridpoem,ng,samp))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9112051.0\n",
            "9034882.0\n",
            "55817.14\n",
            "47308.816\n",
            "608547.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7VidZblYY1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}